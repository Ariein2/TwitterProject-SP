{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('ScientificProg': conda)",
   "display_name": "Python 3.8.5 64-bit ('ScientificProg': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d31d4db3bff70577ea7dc16fe2de9f49cb6e29da5d75c29f919bb3639766ae4e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Markdown text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#%% LOAD PACKAGES\n",
    "import GetOldTweets3 as got\n",
    "import numpy as np\n",
    "import csv, re\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% RETRIEVE TWEETS: \n",
    "\"\"\" \n",
    "### RETRIEVE TWEETS (script): \n",
    "\n",
    "# searchTerm = input(\"Enter Keywords (Space Seperated): \")\n",
    "# start=input(\"Enter Start date in format YYYY-MM-DD: \")\n",
    "# end=input(\"Enter End date in format YYYY-MM-DD: \")\n",
    "\n",
    "\n",
    "searchTerm = \"#COVID19\"\n",
    "start = \"2020-05-10\"\n",
    "end = \"2020-05-11\"\n",
    "\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(searchTerm)\\\n",
    "                                            .setSince(start)\\\n",
    "                                            .setUntil(end)\\\n",
    "                                            .setMaxTweets(50)\n",
    "                                            \n",
    "                                        \n",
    "\n",
    "tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "### RETREIVE TWEETS (Run in terminal)\n",
    "\n",
    "#10-15th May 2020\n",
    "# GetOldTweets3 --querysearch \"#kitten\" --since \"2020-05-10\" --until \"2020-05-15\" --toptweets --maxtweets 10000 --lang en --output \"kitten10M.csv\"\n",
    "# GetOldTweets3 --querysearch \"#pet\" --since \"2020-05-10\" --until \"2020-05-15\" --toptweets --maxtweets 10000 --lang en --output \"pet10M.csv\"\n",
    "# GetOldTweets3 --querysearch \"#COVID19\" --since \"2020-05-10\" --until \"2020-05-15\" --toptweets --maxtweets 10000 --lang en --output \"COVID10M.csv\"\n",
    "# GetOldTweets3 --querysearch \"#BlackLivesMatter\" --since \"2020-05-10\" --until \"2020-05-15\" --toptweets --maxtweets 10000 --lang en --output \"BLM10M.csv\"\n",
    "\n",
    "#26- 31 May 2020\n",
    "# GetOldTweets3 --querysearch \"#BlackLivesMatter\" --since \"2020-05-26\" --until \"2020-05-31\" --toptweets --maxtweets 10000 --lang en --output \"BLM26M.csv\"\n",
    "# GetOldTweets3 --querysearch \"#kitten\" --since \"2020-05-26\" --until \"2020-05-31\" --toptweets --maxtweets 10000 --lang en --output \"kitten26M.csv\"\n",
    "# GetOldTweets3 --querysearch \"#pet\" --since \"2020-05-26\" --until \"2020-05-31\" --toptweets --maxtweets 10000 --lang en --output \"pet26M.csv\" \n",
    "# GetOldTweets3 --querysearch \"#COVID19\" --since \"2020-05-26\" --until \"2020-05-31\" --toptweets --maxtweets 10000 --lang en --output \"COVID26M.csv\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% LOAD DATA FROM CSV FILES\n",
    "\n",
    "# Replace the elements in namefiles with the name of the CSV files to load.\n",
    "# (All files must be located in the same folder as the python script) \n",
    "\n",
    "namefiles = ['BLM10M','BLM26M','COVID10M','COVID26M','kitten10M','kitten26M','pet10M','pet26M']\n",
    "all_data={}\n",
    "\n",
    "for dataset in namefiles: #load all csvs\n",
    "\n",
    "    with open(dataset + '.csv', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        list_reader =list(reader)\n",
    "        all_data[dataset]= list_reader #all files are stored as a list in the all_data dictionary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% PREPROCESSING : \n",
    "# Remove errors in samples loaded. It checks that the date of the tweet is \n",
    "# May 2020, the presence of the # searched in the tweet text and  checks for repeated tweets. \n",
    "# It is necessary to introduce the hashtag associated to each list in the all_data dictionary. \n",
    "# NOTE: This step will always remove at least 1 sample as it corresponds to the first row \n",
    "# from the list of each dataset, which contains the labels of the columns. \n",
    "\n",
    "all_data_removed= {}\n",
    "filtered_data = {}\n",
    "hashtag= ['#BlackLivesMatter','#BlackLivesMatter','#COVID19','#COVID19','#kitten','#kitten','#pet','#pet'] # List of hashtags used in each dataset\n",
    "length_datasets =[]\n",
    "\n",
    "for i, datas in enumerate(all_data):\n",
    "    # Select data to filter\n",
    "    prep_data= pd.DataFrame(all_data[datas])\n",
    "\n",
    "    # Check data is in may of 2020\n",
    "    may_check= prep_data[0].str.contains('2020-05') #check data in May 2020 \n",
    "    filtered_data[datas] = prep_data[may_check] # store filtered dataset\n",
    "    removed_data = prep_data[~may_check] # store samples removed\n",
    "\n",
    "    # Check data contains the # of interest\n",
    "    hashtag_check = filtered_data[datas][6].str.contains(hashtag[i], flags = re.IGNORECASE) #check that data contains # CASE INSENSITIVE\n",
    "    filtered_data [datas] = filtered_data[datas][hashtag_check]\n",
    "    removed_data.append(filtered_data[datas][~hashtag_check])\n",
    "\n",
    "    #Check repeated tweets (using URL): \n",
    "    equal_check = filtered_data[datas][11].duplicated()\n",
    "    filtered_data[datas] = filtered_data[datas][~equal_check]\n",
    "    removed_data.append (filtered_data[datas][equal_check])\n",
    "    all_data_removed [datas] = removed_data \n",
    "\n",
    "    #Display info on samples removed \n",
    "    count_removed = len(removed_data) \n",
    "    length_datasets.append(len(filtered_data[datas])) # Store sample size of each dataset\n",
    "    print('Original size of data ' + datas +': ' + str(len(prep_data)))\n",
    "    print('New size of data ' + datas + ': ' + str(len(filtered_data[datas]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SENTIMENT ANALYSIS: Identify how positive, negative or neutral is a tweet. \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores_all = {}\n",
    "for i, datas in enumerate (filtered_data): #For each dataset\n",
    "    selected = filtered_data[datas]\n",
    "    scores = []\n",
    "    row_names = selected.index.values\n",
    "    for ind, sentence in enumerate(selected[6]): #Check each sentence \n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        scores.append([sentence, selected[3][row_names[ind]], selected[4][row_names[ind]], selected[5][row_names[ind]], vs['neg'], vs['neu'], vs['pos'], vs['compound']])\n",
    "\n",
    "    #Save scores, text and then \n",
    "    scores_all [datas] = pd.DataFrame(scores, columns= ['Text','reply','rts','fav','neg','neu','pos','compound'])\n",
    "\n",
    "print('Sentiment analysis completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% CHECK DATA TO MERGE\n",
    "control_keys = ['kitten10M','pet10M','kitten26M','pet26M']\n",
    "compound_control=[]\n",
    "for val in control_keys:\n",
    "    compound_control.append(scores_all[val]['compound'])\n",
    "\n",
    "# Check if controls are equal. \n",
    "fig, ax= plt.subplots()\n",
    "plt.boxplot(compound_control,labels=control_keys)\n",
    "plt.title('Compound scores CONTROL data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% STATISTICAL TEST?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MERGE DATA \n",
    "scores_all['control10M'] =  pd.concat([scores_all['kitten10M'], scores_all['pet10M']], ignore_index=True)\n",
    "scores_all['control26M'] =  pd.concat([scores_all['kitten26M'], scores_all['pet26M']], ignore_index=True)\n",
    "\n",
    "# Delete old keys (now merged in control 10M and control26M)\n",
    "del scores_all['kitten10M'], scores_all['kitten26M'], scores_all['pet10M'], scores_all['pet26M']\n",
    "\n",
    "# Plot \n",
    "fig, ax= plt.subplots()\n",
    "plt.boxplot([scores_all['control10M']['compound'], scores_all['control10M']['compound']],labels=['control10M','control26M'])\n",
    "plt.title('Compound scores CONTROL data after merging')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DATA SELECTION: Select a subset of data to work with. The subset must be as big as the smallest dataset. \n",
    "# NOTE: REMOVE RANDOM STATE BEFORE RUNNING!!!!! Sets a constant seed. \n",
    "# It also changes the type of the variables reply,rts and fav to numeric\n",
    "min_sample = min(length_datasets)\n",
    "sampled_data={}\n",
    "for data in scores_all:   \n",
    "    sampled_data[data] =  scores_all[data].sample(min_sample,random_state=1) #scores_all[data].sample(min_sample)\n",
    "    sampled_data[data]['reply'] = pd.to_numeric(sampled_data[data]['reply'])\n",
    "    sampled_data[data]['rts'] = pd.to_numeric(sampled_data[data]['rts'])\n",
    "    sampled_data[data]['fav'] = pd.to_numeric(sampled_data[data]['fav'])\n",
    "print('Sampling completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DATA INSPECTION\n",
    "\n",
    "save_comp = []\n",
    "#Plot each sampled datset:neg, neu, pos\n",
    "for data in sampled_data:   \n",
    "    plt.figure()\n",
    "    figs = sampled_data[data].boxplot(column= ['neg','neu','pos'])\n",
    "    save_comp.append(sampled_data[data]['compound'])\n",
    "    plt.title(data + ' all variables boxplot')\n",
    "    plt.show()\n",
    "\n",
    "#Plot compound \n",
    "fig, ax= plt.subplots()\n",
    "ax.axhline(y=0)\n",
    "plt.boxplot(save_comp ,labels= sampled_data.keys())\n",
    "plt.title('Compound scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for data in sampled_data:   \n",
    "    plt.figure()\n",
    "    figs = sampled_data[data].boxplot(column= ['neg','neu','pos'])\n",
    "    save_comp.append(sampled_data[data]['compound'])\n",
    "    plt.title(data + ' all variables boxplot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% TEST RESAMPLING \n",
    "sampled_data={}\n",
    "\n",
    "av_repeated = {}\n",
    "for resamp in range (100):\n",
    "    save_comp_mean=[]\n",
    "    for data in scores_all:   \n",
    "        sampled_data[data] =  scores_all[data].sample(min_sample,) #random_state=1\n",
    "        sampled_data[data]['reply'] = pd.to_numeric(sampled_data[data]['reply'])\n",
    "        sampled_data[data]['rts'] = pd.to_numeric(sampled_data[data]['rts'])\n",
    "        sampled_data[data]['fav'] = pd.to_numeric(sampled_data[data]['fav'])\n",
    "        save_comp_mean.append(np.mean(sampled_data[data]['compound']))\n",
    "    av_repeated[resamp]= save_comp_mean\n",
    "\n",
    "av_repeated2=pd.DataFrame(av_repeated).transpose()\n",
    "fig, ax= plt.subplots()\n",
    "ax.axhline(y=0)\n",
    "av_repeated2.boxplot()\n",
    "plt.title('Compound scores 100 resampling')\n",
    "plt.show()\n",
    "\n",
    "c=1"
   ]
  }
 ]
}